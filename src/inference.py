import torch
import numpy as np
import asyncio
import time
from dataclasses import dataclass
from typing import Optional, List, Any

# Moshi ìŠ¤íƒ€ì¼ ë¡œê±° ì„í¬íŠ¸
try:
    from .client_utils import log, get_logger
except ImportError:
    def log(level, msg): print(f"[{level.upper()}] {msg}")
    def get_logger(): 
        class FallbackLogger:
            def print_token(self, t, color=None): print(t, end="", flush=True)
        return FallbackLogger()

# =============================================================================
# 1. ì„¤ì • ë° ë°ì´í„° í´ë˜ìŠ¤
# =============================================================================
@dataclass
class EngineConfig:
    audio_input_tokens: int = 4   
    text_output_tokens: int = 2   
    audio_output_tokens: int = 4  
    silence_token_id: int = 151646 
    audio_token_id: int = 151675

    system_prompt_text: str = (
        "<|im_start|>system\n"
        "You are a funny comedian performing a stand-up comedy show using Qwen3-Omni.\n"
        "<|im_end|>\n"
    )

# =============================================================================
# 2. ë¡œì§ í´ë˜ìŠ¤ (Stateless Tensor Operations)
# =============================================================================

class Qwen3DuplexLogic:
    def __init__(self, model):
        self.model = model
        self.device = model.device # ëŒ€í‘œ ë””ë°”ì´ìŠ¤ (ë³´í†µ cuda:0)
        
        # [ìˆ˜ì •] ê° ëª¨ë“ˆì˜ ì‹¤ì œ ë””ë°”ì´ìŠ¤ ìœ„ì¹˜ë¥¼ íŒŒì•…í•˜ì—¬ ì €ì¥
        # ëª¨ë¸ì´ ë¶„ì‚°ë˜ì–´ ìˆì„ ê²½ìš° thinker_deviceì™€ talker_deviceê°€ ë‹¤ë¦„
        if hasattr(model, "thinker"):
            self.thinker_device = model.thinker.device
        else:
            self.thinker_device = self.device

        if hasattr(model, "talker"):
            # Talkerì˜ ì²« ë²ˆì§¸ íŒŒë¼ë¯¸í„° ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì¡ìŒ
            self.talker_device = next(model.talker.parameters()).device
        else:
            self.talker_device = self.device
            
        if hasattr(model, "code2wav"):
            self.code2wav_device = next(model.code2wav.parameters()).device
        else:
            self.code2wav_device = self.device

        self.talker_config = model.config.talker_config
        self.num_quantizers = getattr(self.talker_config, "num_quantizers", 16)
        
        try:
            self.audio_dtype = model.thinker.audio_tower.conv2d1.weight.dtype
        except:
            self.audio_dtype = model.dtype

    def _calc_audio_token_count(self, input_lengths):
        """
        [ê³µì‹ ì½”ë“œ Line 99 ê¸°ë°˜]
        Audio Encoderì˜ Convolution ë° Window Chunkingì„ ê³ ë ¤í•œ ì •í™•í•œ í† í° ìˆ˜ ê³„ì‚°
        """
        # 1. ìœˆë„ìš°(100í”„ë ˆì„) ë‹¨ìœ„ë¡œ ì²˜ë¦¬ë˜ì§€ ì•ŠëŠ” ë‚˜ë¨¸ì§€ ë¶€ë¶„ ê³„ì‚°
        input_lengths_leave = input_lengths % 100
        
        # 2. Convolution Layer ì‹œë®¬ë ˆì´ì…˜ (Stride 2, Kernel 3, Padding 1 ì ìš©)
        #    Layer 1
        feat_lengths = (input_lengths_leave - 1) // 2 + 1
        #    Layer 2 & 3 (ë³µí•© ê³µì‹ ì ìš©)
        output_lengths_leave = ((feat_lengths - 1) // 2 + 1 - 1) // 2 + 1
        
        # 3. ì „ì²´ í† í° ìˆ˜ = (ìœˆë„ìš° ê°œìˆ˜ * 13) + ë‚˜ë¨¸ì§€ ë¶€ë¶„ í† í° ìˆ˜
        #    * Qwen3-OmniëŠ” 100í”„ë ˆì„ ìœˆë„ìš° í•˜ë‚˜ë‹¹ 13ê°œì˜ í† í°ì„ ìƒì„±í•©ë‹ˆë‹¤.
        total_output_lengths = output_lengths_leave + (input_lengths // 100) * 13
        
        return int(total_output_lengths)

    @torch.no_grad()
    def thinker_step(self, input_ids, input_features, feature_attention_mask, past_key_values, step_idx):
        # [Safety] Device Move
        target_device = self.thinker_device
        
        if input_ids is not None and input_ids.device != target_device:
            input_ids = input_ids.to(target_device)
        
        # =========================================================================
        # â˜… [ìµœì¢… ìˆ˜ì •] Audio Input ì²˜ë¦¬ (ê³µì‹ ì•„í‚¤í…ì²˜ ì¤€ìˆ˜)
        # =========================================================================
        if input_features is not None:
            if input_features.device != self.thinker_device:
                input_features = input_features.to(self.thinker_device)
            input_features = input_features.to(dtype=self.audio_dtype)

            # [ìˆ˜ì • í¬ì¸íŠ¸ A] ë§ˆìŠ¤í¬ê°€ ì—†ìœ¼ë©´ ê°•ì œë¡œ ìƒì„± (NoneType Error ë°©ì§€)
            if feature_attention_mask is None:
                # shape: [Batch, Mel, Time]
                batch_size = input_features.shape[0]
                time_dim = input_features.shape[2]
                feature_attention_mask = torch.ones(
                    (batch_size, time_dim), 
                    dtype=torch.long, 
                    device=target_device
                )
            else:
                if feature_attention_mask.device != target_device:
                    feature_attention_mask = feature_attention_mask.to(target_device)

            # [ìˆ˜ì • í¬ì¸íŠ¸ B] _calc í•¨ìˆ˜ ëŒ€ì‹  ì‹¤ì œ ëª¨ë¸ì„ ëŒë ¤ì„œ ì •í™•í•œ ì„ë² ë”©ê³¼ ê¸¸ì´ë¥¼ ì–»ìŒ
            # ì´ í•¨ìˆ˜ê°€ Mel Spectrogram -> Audio Embedding ë³€í™˜ì„ ìˆ˜í–‰í•¨
            # ì´ë•Œ ë§ˆìŠ¤í¬ë„ ê°™ì´ ë„£ì–´ì¤˜ì•¼ ì—ëŸ¬ê°€ ì•ˆ ë‚¨
            audio_seq_len = feature_attention_mask.sum(dim=1)
            actual_audio_embeds = self.model.thinker.get_audio_features(
                input_features=input_features,
                feature_attention_mask=feature_attention_mask,
                audio_feature_lengths=audio_seq_len
            )
            
            # [ìˆ˜ì • í¬ì¸íŠ¸ C] ì‹¤ì œ ë‚˜ì˜¨ ì„ë² ë”© ê¸¸ì´ë§Œí¼ input_ids ìƒì„± (Tensor Mismatch í•´ê²°)
            actual_token_count = actual_audio_embeds.shape[1]
            audio_token_id = self.model.config.thinker_config.audio_token_id


            input_ids = torch.full(
                (1, actual_token_count), 
                audio_token_id, 
                dtype=torch.long, 
                device=target_device
            )
            # 5. Transpose í•˜ì§€ ì•ŠìŒ! (Qwen AudioEncoder ë‚´ë¶€ì—ì„œ ì²˜ë¦¬í•¨)
            inputs_embeds = actual_audio_embeds
        elif input_ids is not None:
            # í…ìŠ¤íŠ¸ ì…ë ¥ì¸ ê²½ìš°
            pass
        else:
            # ì˜ˆì™¸ ì²˜ë¦¬
            input_ids = torch.tensor([[0]], device=self.thinker_device)

        # =========================================================================
        
        seq_len = input_ids.shape[1]
        
        # 1. Configì—ì„œ ìµœëŒ€ ê¸¸ì´ ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ 1500 ê¸°ë³¸ê°’)
        # ì˜¤ë””ì˜¤ ì¸ì½”ë”ì˜ í•œê³„(1500)ê°€ ì „ì²´ ë¬¸ë§¥ ê¸¸ì´ë³´ë‹¤ íƒ€ì´íŠ¸í•˜ë¯€ë¡œ ì´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì¡ìŒ
        max_pos_limit = getattr(self.model.config.thinker_config.audio_config, "max_source_positions", 1500)
        
        # 2. Cycling ë¡œì§ ì ìš© (ì•ˆì „ êµ¬ê°„: max_pos_limitì˜ 50% ì§€ì ë¶€í„° ìˆœí™˜)
        # ì˜ˆ: 1500ì´ë©´ 750 ~ 1500 ì‚¬ì´ë¥¼ ë±…ê¸€ë±…ê¸€ ë
        cycle_start = max_pos_limit // 2  # 750
        cycle_len = max_pos_limit - cycle_start # 750
        
        if step_idx >= max_pos_limit:
            safe_start_idx = cycle_start + (step_idx - cycle_start) % cycle_len
        else:
            safe_start_idx = step_idx
            
        current_pos_ids = torch.arange(safe_start_idx, safe_start_idx + seq_len, device=target_device)
        current_pos_ids = current_pos_ids.clamp(0, max_pos_limit - 1)
        position_ids = current_pos_ids.unsqueeze(0).expand(3, -1, -1)


        outputs = self.model.thinker(
            input_ids=input_ids,           # ìœ„ì¹˜ ê³„ì‚°ìš© Placeholder
            inputs_embeds=inputs_embeds,   # â˜… ì‹¤ì œ ì˜¤ë””ì˜¤ ê°’ (ì´ê²Œ ì—†ìœ¼ë©´ ë‚´ë¶€ì—ì„œ ë˜ ê³„ì‚°í•˜ë ¤ë‹¤ ì—ëŸ¬ ë‚¨)
            feature_attention_mask=feature_attention_mask,
            past_key_values=past_key_values,
            position_ids=position_ids,     # ìˆ˜ë™ ê³„ì‚°í•œ ID ì „ë‹¬
            use_cache=True,
            output_hidden_states=True
        )
        # ê¸¸ì´ë¥¼ ê°™ì´ ë°˜í™˜í•˜ì—¬ step_countë¥¼ ì •í™•íˆ ì—…ë°ì´íŠ¸
        return outputs, seq_len

    # @torch.no_grad()
    # def talker_step(self, thinker_hidden, past_key_values, step_idx, input_ids=None):
    #     # log("debug", f"ğŸ” [Check] thinker_hidden: shape={thinker_hidden.shape}, device={thinker_hidden.device}, dtype={thinker_hidden.dtype}")
        
        
    #     # if torch.isnan(thinker_hidden).any():
    #     #     log("error", "ğŸ’€ Critical: Thinker hidden state contains NaN!")
    #     #     raise ValueError("Thinker hidden state is NaN")

    #     # if thinker_hidden.device != self.talker_device:
    #     #     thinker_hidden = thinker_hidden.to(self.talker_device)
        
    #     # if input_ids is None:
    #     #      input_ids = torch.tensor([[self.model.config.talker_config.codec_bos_id]], device=self.talker_device)
    #     # else:
    #     #      input_ids = input_ids.to(self.talker_device)

    #     # 1. ì´ì „ ë‹¨ê³„(Thinker)ì˜ ì—°ì‚°ì´ ì§„ì§œ ëë‚¬ëŠ”ì§€, GPUê°€ ì‚´ì•„ìˆëŠ”ì§€ í™•ì¸


    #     log("debug", "1ï¸âƒ£ Waiting for previous CUDA operations to finish (Synchronize)...")
    #     try:
    #         torch.cuda.synchronize() # ì—¬ê¸°ì„œ ë©ˆì¶”ë©´ -> ë²”ì¸ì€ Thinker_step ì…ë‹ˆë‹¤.
    #         log("debug", "âœ… GPU is alive and synced.")
    #     except Exception as e:
    #         log("error", f"âŒ GPU died BEFORE projection: {e}")
    #         raise e

    #     # 2. ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬ (NaN/Inf)
    #     log("debug", "2ï¸âƒ£ Checking for NaN/Inf in thinker_hidden...")
    #     if torch.isnan(thinker_hidden).any() or torch.isinf(thinker_hidden).any():
    #         log("error", "ğŸ’€ Critical: thinker_hidden contains NaN or Inf!")
    #         # NaNì´ ìˆìœ¼ë©´ ë‹¤ìŒ ì—°ì‚°ì´ ë©ˆì¶œ ìˆ˜ ìˆìŒ
    #         log("debug", f"Sample values: {thinker_hidden[0,0,:10]}")
    #         raise ValueError("NaN detected in thinker output")

    #     # 3. ë ˆì´ì–´ ìì²´ í…ŒìŠ¤íŠ¸ (ë”ë¯¸ ë°ì´í„°)
    #     # ë§Œì•½ thinker_hiddenì— ë­”ê°€ ë¬¸ì œê°€ ìˆë‹¤ë©´ ë”ë¯¸ëŠ” í†µê³¼í•˜ê³  ì‹¤ë°ì´í„°ëŠ” ë©ˆì¶œ ê²ƒì„
    #     log("debug", "3ï¸âƒ£ Running Dummy Projection Test...")
    #     try:
    #         dummy_input = torch.randn_like(thinker_hidden)
    #         conditioned_hidden = self.model.talker.text_projection(dummy_input)
    #         log("debug", "âœ… Dummy Projection passed (Layer is fine).")
    #     except Exception as e:
    #         log("error", f"âŒ Layer weights might be corrupted: {e}")
    #         raise e

    #     # =====================================================================
    #     # ğŸš€ ì‹¤ì œ ë¡œì§ ì‹¤í–‰
    #     # =====================================================================
        
    #     # 1. Device Move
    #     if thinker_hidden.device != self.talker_device:
    #         thinker_hidden = thinker_hidden.to(self.talker_device)
        
    #     if input_ids is None:
    #          input_ids = torch.tensor([[self.model.config.talker_config.codec_bos_id]], device=self.talker_device)
    #     else:
    #          input_ids = input_ids.to(self.talker_device)

    #     2. Real Projection (ì—¬ê¸°ì„œ ë©ˆì¶”ë©´ ë°ì´í„° íŠ¹ì´ì„± ë¬¸ì œ)
       
    #     conditioned_hidden = self.model.talker.text_projection(thinker_hidden)
    #     log("debug","finishing projection ") 

    #     ##finshing debugging
    #     audio_embed = self.model.talker.model.get_input_embeddings()(input_ids)
    #     talker_inputs_embeds = audio_embed + conditioned_hidden
        
    #     position_ids = torch.tensor([[step_idx]], device=self.talker_device)
    #     position_ids = position_ids.unsqueeze(0).expand(3, -1, -1)


    #     log("debug", f"Talker Step {step_idx}: Input Embeds Shape: {talker_inputs_embeds.shape}")
    #     talker_out = self.model.talker.model(
    #         inputs_embeds=talker_inputs_embeds,
    #         past_key_values=past_key_values,
    #         position_ids=position_ids,
    #         use_cache=True
    #     )
        
    #     logits = self.model.talker.codec_head(talker_out.last_hidden_state[:, -1, :])
    #     layer0_code = logits.argmax(dim=-1, keepdim=True)
        
    #     last_id_hidden = self.model.talker.get_input_embeddings()(layer0_code)
    #     past_hidden = talker_out.last_hidden_state[:, -1:]
    #     predictor_input = torch.cat((past_hidden, last_id_hidden), dim=1)
        
    #     needed_tokens = self.num_quantizers - 1
        
    #     predictor_out = self.model.talker.code_predictor.generate(
    #         inputs_embeds=predictor_input,
    #         max_new_tokens=needed_tokens, 
    #         do_sample=False
    #     )
        
    #     full_audio_codes = torch.cat([layer0_code, predictor_out], dim=1)
    #     return full_audio_codes, talker_out.past_key_values
    @torch.no_grad()
    def talker_step(self, thinker_hidden, past_key_values, step_idx, input_ids=None):
        try:
            target_device = self.talker_device
            
            # 1. Device & Memory Safety Check
            if thinker_hidden.device != target_device:
                thinker_hidden = thinker_hidden.to(target_device)
            if not thinker_hidden.is_contiguous():
                thinker_hidden = thinker_hidden.contiguous()

            # 2. Projection (ê°€ì¥ ë§ì´ ë©ˆì¶”ëŠ” êµ¬ê°„)
            #    ì—¬ê¸°ì„œ ë©ˆì¶”ì§€ ì•Šê²Œ íƒ€ì„ì•„ì›ƒì„ ê±¸ ìˆ˜ëŠ” ì—†ìœ¼ë‹ˆ, ì—ëŸ¬ê°€ ë‚˜ë©´ ë”ë¯¸ë¡œ ëŒ€ì²´í•˜ëŠ” êµ¬ì¡°ëŠ” ì•„ë‹ˆì§€ë§Œ,
            #    ìµœì†Œí•œ í™•ì‹¤í•˜ê²Œ ì‹¤í–‰ë˜ë„ë¡ êµ¬ì„±
            conditioned_hidden = self.model.talker.text_projection(thinker_hidden)
            
            # 3. Main Forward
            if input_ids is None:
                 input_ids = torch.tensor([[self.model.config.talker_config.codec_bos_id]], device=self.talker_device)
            else:
                 input_ids = input_ids.to(self.talker_device)

            audio_embed = self.model.talker.model.get_input_embeddings()(input_ids)
            talker_inputs_embeds = audio_embed + conditioned_hidden
            
            max_pos_limit = getattr(self.model.config.talker_config.text_config, "max_position_embeddings", 2048)
            
            # TalkerëŠ” ì˜¤ë””ì˜¤ ì¸ì½”ë” ì œì•½ì´ ì—†ì–´ì„œ ì¢€ ë” ê¸¸ ìˆ˜ ìˆì§€ë§Œ, 
            # ì•ˆì „í•˜ê²Œ 1500~2000 ì‚¬ì´ ì ë‹¹í•œ ê°’ìœ¼ë¡œ ìˆœí™˜ (Thinkerì™€ ë¹„ìŠ·í•˜ê²Œ ë§ì¶”ëŠ” ê²Œ ì¢‹ìŒ)
            if max_pos_limit > 1500: max_pos_limit = 1500 # ë³´ìˆ˜ì  ì„¤ì •
            
            cycle_start = max_pos_limit // 2
            cycle_len = max_pos_limit - cycle_start
            
            if step_idx >= max_pos_limit:
                safe_step_idx = cycle_start + (step_idx - cycle_start) % cycle_len
            else:
                safe_step_idx = step_idx
                
            if safe_step_idx >= max_pos_limit:
                safe_step_idx = max_pos_limit - 1
                
            position_ids = torch.tensor([[safe_step_idx]], device=target_device)
            position_ids = position_ids.unsqueeze(0).expand(3, -1, -1)

            talker_out = self.model.talker.model(
                inputs_embeds=talker_inputs_embeds,
                past_key_values=past_key_values,
                position_ids=position_ids,
                use_cache=True
            )

            # 4. Code Prediction (Manual Loop)
            #    ì—¬ê¸°ê°€ ë„ˆë¬´ ëŠë¦¬ê±°ë‚˜ ë©ˆì¶”ë©´ ë°”ë¡œ Skipí•˜ê¸° ìœ„í•´ try-except ë¸”ë¡ ê°•í™”
            logits = self.model.talker.codec_head(talker_out.last_hidden_state[:, -1, :])
            layer0_code = logits.argmax(dim=-1, keepdim=True)
            
            # --- [BYPASS START] ë³µì¡í•œ Predictor ë¡œì§ ëŒ€ì‹  ë‹¨ìˆœí™” ---
            # ë§Œì•½ ì—¬ê¸°ì„œë„ ë©ˆì¶˜ë‹¤ë©´ ì•„ë˜ ì „ì²´ ë£¨í”„ë¥¼ ì£¼ì„ ì²˜ë¦¬í•˜ê³  
            # full_audio_codes = torch.randint(0, 1024, (1, 8), device=self.talker_device) ë¡œ ëŒ€ì²´ ê°€ëŠ¥
            
            last_id_hidden = self.model.talker.get_input_embeddings()(layer0_code)
            past_hidden = talker_out.last_hidden_state[:, -1:]
            predictor_input = torch.cat((past_hidden, last_id_hidden), dim=1)
            
            predictor_codes = [layer0_code]
            predictor_kv = None 
            
            for i in range(self.num_quantizers - 1):
                # Predictor Forward
                pred_out = self.model.talker.code_predictor.model(
                    inputs_embeds=predictor_input,
                    past_key_values=predictor_kv,
                    use_cache=True
                )
                predictor_kv = pred_out.past_key_values
                
                # â˜…â˜…â˜… [ìˆ˜ì •ëœ ë¶€ë¶„] ì£¼ì†Œ ë³€ê²½: talker.lm_head -> talker.code_predictor.lm_head â˜…â˜…â˜…
                # Qwen3 êµ¬ì¡°ìƒ Residual Layer ì˜ˆì¸¡ í—¤ë“œëŠ” code_predictor ì•ˆì— ìˆìŠµë‹ˆë‹¤.
                curr_logits = self.model.talker.code_predictor.lm_head[i](pred_out.last_hidden_state[:, -1, :])
                
                next_code = curr_logits.argmax(dim=-1, keepdim=True)
                predictor_codes.append(next_code)
                
                # ë‹¤ìŒ ì…ë ¥ ì„ë² ë”©
                predictor_input = self.model.talker.code_predictor.get_input_embeddings()[i](next_code)
            
            full_audio_codes = torch.cat(predictor_codes, dim=1)
            return full_audio_codes, talker_out.past_key_values

        except Exception as e:
            # ğŸš¨ [EMERGENCY BYPASS] ë¬´ìŠ¨ ì—ëŸ¬ê°€ ë‚˜ë“  ë©ˆì¶”ì§€ ì•Šê²Œ ê°€ì§œ ë°ì´í„° ë¦¬í„´
            log("error", f"ğŸš¨ Talker Crashed! Returning Dummy Data. Error: {e}")
            
            # ë”ë¯¸ ì˜¤ë””ì˜¤ ì½”ë“œ (ëœë¤)
            dummy_codes = torch.randint(0, 1024, (1, self.num_quantizers), device=self.talker_device)
            # ë”ë¯¸ KV Cache (ê·¸ëƒ¥ None ì£¼ë©´ ë‹¤ìŒ ìŠ¤í…ì—ì„œ ì—ëŸ¬ë‚  ìˆ˜ ìˆìœ¼ë‹ˆ, ì´ì „êº¼ ë¦¬í„´í•˜ê±°ë‚˜ None)
            # ì—¬ê¸°ì„œëŠ” ì•ˆì „í•˜ê²Œ None ë¦¬í„´ (ëª¨ë¸ì´ ì•Œì•„ì„œ ì²˜ë¦¬í•˜ê²Œ ë‘ , ì„±ëŠ¥ì€ ë§ê°€ì§€ì§€ë§Œ ë£¨í”„ëŠ” ëˆë‹¤)
            return dummy_codes, past_key_values

    @torch.no_grad()
    def decode_audio(self, audio_codes: torch.Tensor) -> np.ndarray:
        # [Device Alignment] Code2Wavê°€ ìˆëŠ” ê³³ìœ¼ë¡œ ì´ë™
        target_device = self.code2wav_device
        
        if audio_codes.device != target_device:
            audio_codes = audio_codes.to(target_device)
            
        if audio_codes.dim() == 2:
            audio_codes = audio_codes.unsqueeze(-1)
            
        wav_tensor = self.model.code2wav(audio_codes)
        wav_cpu = wav_tensor.to("cpu", non_blocking=True).float().numpy()
        return wav_cpu

# =============================================================================
# 3. ì—”ì§„ í´ë˜ìŠ¤ (Asyncio + Executor)
# =============================================================================
class Qwen3OmniFullDuplexEngine:
    def __init__(self, model, tokenizer, config: EngineConfig):
        self.model = model
        self.tokenizer = tokenizer
        self.cfg = config
        self.logic = Qwen3DuplexLogic(model)
        
        self.input_queue = None
        self.hidden_queue = None
        self.output_queue = None
        
        self.thinker_kv_cache = None
        self.talker_kv_cache = None
        self.last_talker_token = None
        
        self.thinker_step_count = 0
        self.talker_step_count = 0
        
        self.is_running = False
        self.thinker_task = None
        self.talker_task = None

    async def initialize(self):
        log("info", "Initializing Async Engine...")
        self.input_queue = asyncio.Queue()
        self.hidden_queue = asyncio.Queue()
        self.output_queue = asyncio.Queue()

        # System Prompt Tokenize
        initial_ids = self.tokenizer(
            self.cfg.system_prompt_text, 
            return_tensors="pt", 
            add_special_tokens=False
        ).input_ids.to(self.logic.thinker_device)
        
        # Talker ì´ˆê¸° í† í° ì„¤ì •
        codec_bos = self.model.config.talker_config.codec_bos_id
        self.last_talker_token = torch.tensor([[codec_bos]], device=self.logic.talker_device)

        # Prefill (Blocking OK here)
        with torch.no_grad():
            # â˜… [ìˆ˜ì • í¬ì¸íŠ¸] ë¦¬í„´ê°’ì´ (outputs, seq_len) íŠœí”Œì´ë¯€ë¡œ ì–¸íŒ¨í‚¹ í•´ì•¼ í•¨
            out, _ = self.logic.thinker_step(
                input_ids=initial_ids, 
                input_features=None, 
                feature_attention_mask=None,
                past_key_values=None, 
                step_idx=0
            )
            
            # ì´ì œ outì€ ì •ìƒì ì¸ ModelOutput ê°ì²´ì…ë‹ˆë‹¤.
            self.thinker_kv_cache = out.past_key_values
            self.thinker_step_count = initial_ids.shape[1]
            
        log("info", "Engine Ready.")
        
    async def _thinker_loop(self):
        loop = asyncio.get_running_loop()
        
        while self.is_running:
            audio_features = await self.input_queue.get()
            
            def run_thinker_inference():
                with torch.no_grad():
                    # =========================================================
                    # [Step 1] ë“£ê¸° (Listening)
                    # =========================================================
                    thinker_out, consumed_len = self.logic.thinker_step(
                        input_ids=None, 
                        input_features=audio_features,
                        feature_attention_mask=None,
                        past_key_values=self.thinker_kv_cache,
                        step_idx=self.thinker_step_count
                    )
                    
                    self.thinker_kv_cache = thinker_out.past_key_values
                    self.thinker_step_count += consumed_len 

                    # =========================================================
                    # [Step 2] íŒë‹¨ (Decision)
                    # =========================================================
                    next_token = thinker_out.logits[:, -1, :].argmax(dim=-1, keepdim=True)
                    token_id = next_token.item()

                    log("debug", f"Thinker predicted token ID: {token_id}")
                    # if token_id == self.cfg.silence_token_id or token_id == 151645:
                    #      return None, "<|silence|>"

                    # =========================================================
                    # [Step 3] ë§í•˜ê¸° (Speaking) - â˜… ìˆ˜ì •ëœ ë¶€ë¶„
                    # =========================================================
                    current_turn_hiddens = []
                    
                    # [ì‚­ì œë¨] current_turn_hiddens.append(thinker_out.hidden_states[-1]) 
                    # ì´ìœ : ìœ„ ì½”ë“œëŠ” 'ì˜¤ë””ì˜¤'ì— ëŒ€í•œ íˆë“  ìŠ¤í…Œì´íŠ¸ì´ë¯€ë¡œ Talkerì—ê²Œ ì í•©í•˜ì§€ ì•ŠìŒ.
                    
                    # [ìˆ˜ì •] ì„¤ì •ëœ í† í° ìˆ˜(ì˜ˆ: 2)ë§Œí¼ ë£¨í”„ë¥¼ ëŒë©° "ìˆœìˆ˜ í…ìŠ¤íŠ¸ íˆë“  ìŠ¤í…Œì´íŠ¸" ìƒì„±
                    token_str = ""
                    
                    # range(N - 1) -> range(N)ìœ¼ë¡œ ë³€ê²½
                    for _ in range(self.cfg.text_output_tokens):
                        # 1. ì˜ˆì¸¡ëœ í…ìŠ¤íŠ¸ í† í°(next_token)ì„ ì…ë ¥ìœ¼ë¡œ ë‹¤ì‹œ Thinker ì‹¤í–‰
                        thinker_out, _ = self.logic.thinker_step(
                            input_ids=next_token,
                            input_features=None,
                            feature_attention_mask=None,
                            past_key_values=self.thinker_kv_cache,
                            step_idx=self.thinker_step_count
                        )
                        
                        # 2. ìƒíƒœ ì—…ë°ì´íŠ¸
                        self.thinker_kv_cache = thinker_out.past_key_values
                        self.thinker_step_count += 1
                        
                        # 3. â˜… ì¤‘ìš”: í…ìŠ¤íŠ¸ ì…ë ¥ì— ëŒ€í•œ ê²°ê³¼(Hidden State)ë§Œ ì €ì¥
                        #    ì´ê²ƒì´ ê³µì‹ ì½”ë“œì˜ assistant_hidden ë¶€ë¶„ê³¼ ì¼ì¹˜í•¨
                        safe_hidden = thinker_out.hidden_states[-1].detach().clone()


                        current_turn_hiddens.append(safe_hidden)
                        
                        # 4. ë‹¤ìŒ í† í° ì˜ˆì¸¡ ë° ë¡œê·¸ ì¤€ë¹„
                        next_token = thinker_out.logits[:, -1, :].argmax(dim=-1, keepdim=True)
                        token_str += self.tokenizer.decode([next_token.item()])
                    
                    final_hidden_to_send = torch.cat(current_turn_hiddens, dim=1).contiguous()
                    # ê²°ê³¼: [1, 2, Hidden_Dim] (ì˜¤ë””ì˜¤ ì„ì´ì§€ ì•Šì€ ìˆœìˆ˜ í…ìŠ¤íŠ¸ ìƒíƒœ)
                    return final_hidden_to_send, token_str

            # Executor ì‹¤í–‰ (Senderë¥¼ ë°©í•´í•˜ì§€ ì•ŠìŒ)
            stacked_hidden, log_str = await loop.run_in_executor(None, run_thinker_inference)
            
            # ì‹¤ì‹œê°„ í† í° ë¡œê·¸ ì¶œë ¥
            get_logger().print_token(log_str)

            # â˜… [ê²°ê³¼ ì²˜ë¦¬] Hidden Stateê°€ ìˆì„ ë•Œë§Œ(Silenceê°€ ì•„ë‹ ë•Œë§Œ) íì— ë„£ìŒ
            if stacked_hidden is not None:
                await self.hidden_queue.put(stacked_hidden)
            else:
                # Silenceì¸ ê²½ìš°: íì— ë„£ì§€ ì•Šê³  ë£¨í”„ ì²˜ìŒìœ¼ë¡œ ëŒì•„ê° (ë‹¤ìŒ ì˜¤ë””ì˜¤ ëŒ€ê¸°)
                # í•˜ì§€ë§Œ KV CacheëŠ” ì´ë¯¸ ì—…ë°ì´íŠ¸ ë˜ì—ˆìœ¼ë¯€ë¡œ ë¬¸ë§¥ì€ ì´ì–´ì§
                pass

    async def _talker_loop(self):
        log("info", "Talker Loop Started")
        loop = asyncio.get_running_loop()
        
        while self.is_running:
            # íì—ì„œ ë°ì´í„°ë¥¼ êº¼ë‚¼ ë•Œê¹Œì§€ ëŒ€ê¸°
            source_hidden = await self.hidden_queue.get()
            
            def run_talker_inference():
                with torch.no_grad():
                    num_hiddens = source_hidden.shape[1]
                    ratio = self.cfg.audio_output_tokens // self.cfg.text_output_tokens
                    output_chunks = []

            

                    for i in range(num_hiddens):
                        one_hidden = source_hidden[:, i:i+1, :]
                        for _ in range(ratio):
            
                            codes, new_kv = self.logic.talker_step(
                                thinker_hidden=one_hidden,
                                past_key_values=self.talker_kv_cache,
                                step_idx=self.talker_step_count,
                                input_ids=self.last_talker_token
                            )
                            self.talker_kv_cache = new_kv
                            self.talker_step_count += 1
                            self.last_talker_token = codes[:, 0:1] 
                            
                            wav_np = self.logic.decode_audio(codes)
                            output_chunks.append(wav_np)
                    return output_chunks

            # GPU ì—°ì‚° ìˆ˜í–‰
            wav_chunks_np = await loop.run_in_executor(None, run_talker_inference)
            
            # ê²°ê³¼ ì „ì†¡
            for wav_np in wav_chunks_np:
                wav_int16 = (wav_np * 32767).astype(np.int16).tobytes()
                await self.output_queue.put(wav_int16)

    async def start(self):
        if self.is_running: return
        self.is_running = True
        await self.initialize()
        self.thinker_task = asyncio.create_task(self._thinker_loop())
        self.talker_task = asyncio.create_task(self._talker_loop())
        log("info", "Engine Started (Async + Executor)")

    async def stop(self):
        self.is_running = False
        if self.thinker_task: self.thinker_task.cancel()
        if self.talker_task: self.talker_task.cancel()
        log("info", "Engine Stopped")

    async def push_audio(self, audio_features: torch.Tensor):
        await self.input_queue.put(audio_features)

    async def get_audio_output(self) -> Optional[bytes]:
        try:
            return self.output_queue.get_nowait()
        except asyncio.QueueEmpty:
            return None
