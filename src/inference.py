import torch
import numpy as np
import asyncio
import time
from dataclasses import dataclass
from typing import Optional, List, Any

# Moshi ìŠ¤íƒ€ì¼ ë¡œê±° ì„í¬íŠ¸
try:
    from .client_utils import log, get_logger
except ImportError:
    def log(level, msg): print(f"[{level.upper()}] {msg}")
    def get_logger(): 
        class FallbackLogger:
            def print_token(self, t, color=None): print(t, end="", flush=True)
        return FallbackLogger()

# =============================================================================
# 1. ì„¤ì • ë° ë°ì´í„° í´ë˜ìŠ¤
# =============================================================================
@dataclass
class EngineConfig:
    audio_input_tokens: int = 4   
    text_output_tokens: int = 2   
    audio_output_tokens: int = 4  
    silence_token_id: int = 151646 
    
    system_prompt_text: str = (
        "<|im_start|>system\n"
        "You are a funny comedian performing a stand-up comedy show using Qwen3-Omni.\n"
        "<|im_end|>\n"
    )

# =============================================================================
# 2. ë¡œì§ í´ë˜ìŠ¤ (Stateless Tensor Operations)
# =============================================================================
class Qwen3DuplexLogic:
    def __init__(self, model):
        self.model = model
        self.device = model.device
        
        self.thinker_device = model.thinker.device
        self.talker_device = model.talker.device
        self.code2wav_device = model.code2wav.device
        
        self.talker_config = model.config.talker_config
        self.num_quantizers = getattr(self.talker_config, "num_quantizers", 16)
        
        try:
            self.audio_dtype = model.thinker.audio_tower.conv2d1.weight.dtype
        except:
            self.audio_dtype = model.dtype

    @torch.no_grad()
    def thinker_step(self, input_ids, input_features, feature_attention_mask, past_key_values, step_idx):
        # [Device Move]
        if input_ids is not None and input_ids.device != self.thinker_device:
            input_ids = input_ids.to(self.thinker_device)
        if input_features is not None:
            if input_features.device != self.thinker_device:
                input_features = input_features.to(self.thinker_device)
            input_features = input_features.to(dtype=self.audio_dtype)
        if feature_attention_mask is not None and feature_attention_mask.device != self.thinker_device:
            feature_attention_mask = feature_attention_mask.to(self.thinker_device)

        # [Dummy Token Logic]
        if input_ids is None and input_features is not None:
            input_ids = torch.tensor([[0]], device=self.thinker_device)

        position_ids = torch.tensor([[step_idx]], device=self.thinker_device)
        position_ids = position_ids.unsqueeze(0).expand(3, -1, -1)

        outputs = self.model.thinker(
            input_ids=input_ids,
            input_features=input_features,
            feature_attention_mask=feature_attention_mask,
            past_key_values=past_key_values,
            position_ids=position_ids,
            use_cache=True,
            output_hidden_states=True
        )
        return outputs

    @torch.no_grad()
    def talker_step(self, thinker_hidden, past_key_values, step_idx, input_ids=None):
        if thinker_hidden.device != self.talker_device:
            thinker_hidden = thinker_hidden.to(self.talker_device)
        
        if input_ids is None:
             input_ids = torch.tensor([[self.model.config.talker_config.codec_bos_id]], device=self.talker_device)
        else:
             input_ids = input_ids.to(self.talker_device)

        conditioned_hidden = self.model.talker.text_projection(thinker_hidden)
        audio_embed = self.model.talker.model.get_input_embeddings()(input_ids)
        talker_inputs_embeds = audio_embed + conditioned_hidden
        
        position_ids = torch.tensor([[step_idx]], device=self.talker_device)
        position_ids = position_ids.unsqueeze(0).expand(3, -1, -1)

        talker_out = self.model.talker.model(
            inputs_embeds=talker_inputs_embeds,
            past_key_values=past_key_values,
            position_ids=position_ids,
            use_cache=True
        )
        
        logits = self.model.talker.codec_head(talker_out.last_hidden_state[:, -1, :])
        layer0_code = logits.argmax(dim=-1, keepdim=True)
        
        last_id_hidden = self.model.talker.get_input_embeddings()(layer0_code)
        past_hidden = talker_out.last_hidden_state[:, -1:]
        predictor_input = torch.cat((past_hidden, last_id_hidden), dim=1)
        
        needed_tokens = self.num_quantizers - 1
        
        predictor_out = self.model.talker.code_predictor.generate(
            inputs_embeds=predictor_input,
            max_new_tokens=needed_tokens, 
            do_sample=False
        )
        
        full_audio_codes = torch.cat([layer0_code, predictor_out], dim=1)
        return full_audio_codes, talker_out.past_key_values

    @torch.no_grad()
    def decode_audio(self, audio_codes: torch.Tensor) -> np.ndarray:
        if audio_codes.device != self.code2wav_device:
            audio_codes = audio_codes.to(self.code2wav_device)
        if audio_codes.dim() == 2:
            audio_codes = audio_codes.unsqueeze(-1)
            
        wav_tensor = self.model.code2wav(audio_codes)
        # Non-blocking transfer
        wav_cpu = wav_tensor.to("cpu", non_blocking=True).float().numpy()
        return wav_cpu

# =============================================================================
# 3. ì—”ì§„ í´ë˜ìŠ¤ (Asyncio + Executor)
# =============================================================================
class Qwen3OmniFullDuplexEngine:
    def __init__(self, model, tokenizer, config: EngineConfig):
        self.model = model
        self.tokenizer = tokenizer
        self.cfg = config
        self.logic = Qwen3DuplexLogic(model)
        
        self.input_queue = None
        self.hidden_queue = None
        self.output_queue = None
        
        self.thinker_kv_cache = None
        self.talker_kv_cache = None
        self.last_talker_token = None
        
        self.thinker_step_count = 0
        self.talker_step_count = 0
        
        self.is_running = False
        self.thinker_task = None
        self.talker_task = None

    async def initialize(self):
        log("info", "Initializing Async Engine...")
        self.input_queue = asyncio.Queue()
        self.hidden_queue = asyncio.Queue()
        self.output_queue = asyncio.Queue()

        initial_ids = self.tokenizer(
            self.cfg.system_prompt_text, 
            return_tensors="pt", 
            add_special_tokens=False
        ).input_ids.to(self.logic.thinker_device)
        
        codec_bos = self.model.config.talker_config.codec_bos_id
        self.last_talker_token = torch.tensor([[codec_bos]], device=self.logic.talker_device)

        # Prefill (Blocking OK here)
        with torch.no_grad():
            out = self.logic.thinker_step(
                input_ids=initial_ids, input_features=None, feature_attention_mask=None,
                past_key_values=None, step_idx=0
            )
            self.thinker_kv_cache = out.past_key_values
            self.thinker_step_count = initial_ids.shape[1]
            
        log("info", "Engine Ready.")
        
    async def _thinker_loop(self):
        log("info", "Thinker Loop Started")
        loop = asyncio.get_running_loop()
        
        while self.is_running:
            # 1. ì˜¤ë””ì˜¤ ì…ë ¥ ëŒ€ê¸°
            audio_features = await self.input_queue.get()
            
            # 2. GPU ì—°ì‚° (Blocking ë°©ì§€ë¥¼ ìœ„í•´ Executor ì‚¬ìš©)
            def run_thinker_inference():
                with torch.no_grad():
                    # =========================================================
                    # [Step 1] Audio Processing (ë“£ê¸°)
                    # =========================================================
                    time_len = audio_features.shape[2]
                    feature_mask = torch.ones((1, time_len), device=self.logic.thinker_device, dtype=torch.long)

                    thinker_out = self.logic.thinker_step(
                        input_ids=None, 
                        input_features=audio_features,
                        feature_attention_mask=feature_mask,
                        past_key_values=self.thinker_kv_cache,
                        step_idx=self.thinker_step_count
                    )
                    
                    # â˜… [ì¤‘ìš”] ë“£ê¸° ê³¼ì •ì˜ KV Cache ì—…ë°ì´íŠ¸ (ë¬´ì¡°ê±´ ìˆ˜í–‰)
                    self.thinker_kv_cache = thinker_out.past_key_values
                    self.thinker_step_count += 4 

                    # =========================================================
                    # [Step 2] First Token Generation (íŒë‹¨)
                    # =========================================================
                    next_token = thinker_out.logits[:, -1, :].argmax(dim=-1, keepdim=True)
                    token_id = next_token.item()
                    
                    # ë¡œê·¸ìš© ë¬¸ìì—´ ë¯¸ë¦¬ ë””ì½”ë”©
                    if token_id == self.cfg.silence_token_id:
                        token_str = "<|silence|>"
                    elif token_id == 151645:
                        token_str = "<|im_end|>"
                    else:
                        token_str = self.tokenizer.decode([token_id], skip_special_tokens=False)

                    # # â˜… [í•µì‹¬ ë¡œì§] Silence Check
                    if token_id == self.cfg.silence_token_id :
                        # 1. KV CacheëŠ” ìœ„ì—ì„œ ì´ë¯¸ ì—…ë°ì´íŠ¸ ë˜ì—ˆìœ¼ë¯€ë¡œ ê¸°ì–µì€ ìœ ì§€ë¨.
                        # 2. Talkerë¡œ ë³´ë‚¼ Hidden StateëŠ” ì—†ìŒ.
                        # 3. ì—¬ê¸°ì„œ í•¨ìˆ˜ ì¢…ë£Œ (Talker Queueì— ë„£ì§€ ì•ŠìŒ)
                        return None, token_str

                    
                    # =========================================================
                    # [Step 3] Text Generation (ë§í•˜ê¸° ê²°ì‹¬í–ˆì„ ë•Œë§Œ)
                    # =========================================================
                    current_turn_hiddens = []
                    current_turn_hiddens.append(thinker_out.hidden_states[-1])
                    
                    # ì„¤ì •ëœ í† í° ìˆ˜ë§Œí¼ ì¶”ê°€ ìƒì„±
                    for _ in range(self.cfg.text_output_tokens - 1):
                        thinker_out = self.logic.thinker_step(
                            input_ids=next_token,
                            input_features=None,
                            feature_attention_mask=None,
                            past_key_values=self.thinker_kv_cache,
                            step_idx=self.thinker_step_count
                        )
                        # ìƒì„±í•˜ë©´ì„œ Cache ê³„ì† ì—…ë°ì´íŠ¸
                        self.thinker_kv_cache = thinker_out.past_key_values
                        self.thinker_step_count += 1
                        
                        next_token = thinker_out.logits[:, -1, :].argmax(dim=-1, keepdim=True)
                        token_str += self.tokenizer.decode([next_token.item()])
                        
                        current_turn_hiddens.append(thinker_out.hidden_states[-1])
                    
                    # Talkerì—ê²Œ ë³´ë‚¼ Hidden State ë¬¶ìŒ ë°˜í™˜
                    return torch.cat(current_turn_hiddens, dim=1), token_str

            # Executor ì‹¤í–‰ (Senderë¥¼ ë°©í•´í•˜ì§€ ì•ŠìŒ)
            stacked_hidden, log_str = await loop.run_in_executor(None, run_thinker_inference)
            
            # ì‹¤ì‹œê°„ í† í° ë¡œê·¸ ì¶œë ¥
            get_logger().print_token(log_str)

            # â˜… [ê²°ê³¼ ì²˜ë¦¬] Hidden Stateê°€ ìˆì„ ë•Œë§Œ(Silenceê°€ ì•„ë‹ ë•Œë§Œ) íì— ë„£ìŒ
            if stacked_hidden is not None:
                await self.hidden_queue.put(stacked_hidden)
            else:
                # Silenceì¸ ê²½ìš°: íì— ë„£ì§€ ì•Šê³  ë£¨í”„ ì²˜ìŒìœ¼ë¡œ ëŒì•„ê° (ë‹¤ìŒ ì˜¤ë””ì˜¤ ëŒ€ê¸°)
                # í•˜ì§€ë§Œ KV CacheëŠ” ì´ë¯¸ ì—…ë°ì´íŠ¸ ë˜ì—ˆìœ¼ë¯€ë¡œ ë¬¸ë§¥ì€ ì´ì–´ì§
                pass

    async def _talker_loop(self):
        log("info", "Talker Loop Started")
        loop = asyncio.get_running_loop()
        
        while self.is_running:
            # íì—ì„œ ë°ì´í„°ë¥¼ êº¼ë‚¼ ë•Œê¹Œì§€ ëŒ€ê¸°
            source_hidden = await self.hidden_queue.get()
            
            # â˜… [ìš”ì²­í•˜ì‹  ìˆ˜ì •] Talkerê°€ ì‹¤ì œë¡œ ì¼ì„ ì‹œì‘í•  ë•Œ ë¡œê·¸ ì¶œë ¥
            # (Queueì—ì„œ êº¼ëƒˆë‹¤ëŠ” ê±´ ì¹¨ë¬µì´ ì•„ë‹ˆë¼ëŠ” ëœ»)
            log("info", "ğŸ‘„ Talker generating audio...")
            
            def run_talker_inference():
                with torch.no_grad():
                    num_hiddens = source_hidden.shape[1]
                    ratio = self.cfg.audio_output_tokens // self.cfg.text_output_tokens
                    output_chunks = []
                    
                    for i in range(num_hiddens):
                        one_hidden = source_hidden[:, i:i+1, :]
                        for _ in range(ratio):
                            codes, new_kv = self.logic.talker_step(
                                thinker_hidden=one_hidden,
                                past_key_values=self.talker_kv_cache,
                                step_idx=self.talker_step_count,
                                input_ids=self.last_talker_token
                            )
                            self.talker_kv_cache = new_kv
                            self.talker_step_count += 1
                            self.last_talker_token = codes[:, 0:1] 
                            
                            wav_np = self.logic.decode_audio(codes)
                            output_chunks.append(wav_np)
                    return output_chunks

            # GPU ì—°ì‚° ìˆ˜í–‰
            wav_chunks_np = await loop.run_in_executor(None, run_talker_inference)
            
            # ê²°ê³¼ ì „ì†¡
            for wav_np in wav_chunks_np:
                wav_int16 = (wav_np * 32767).astype(np.int16).tobytes()
                await self.output_queue.put(wav_int16)

    async def start(self):
        if self.is_running: return
        self.is_running = True
        await self.initialize()
        self.thinker_task = asyncio.create_task(self._thinker_loop())
        self.talker_task = asyncio.create_task(self._talker_loop())
        log("info", "Engine Started (Async + Executor)")

    async def stop(self):
        self.is_running = False
        if self.thinker_task: self.thinker_task.cancel()
        if self.talker_task: self.talker_task.cancel()
        log("info", "Engine Stopped")

    async def push_audio(self, audio_features: torch.Tensor):
        await self.input_queue.put(audio_features)

    async def get_audio_output(self) -> Optional[bytes]:
        try:
            return self.output_queue.get_nowait()
        except asyncio.QueueEmpty:
            return None