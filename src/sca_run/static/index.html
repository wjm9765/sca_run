<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>sca_run mic</title>
  <style>
    body { font-family: system-ui, -apple-system, Segoe UI, Roboto, sans-serif; margin: 24px; }
    button { padding: 10px 14px; margin-right: 10px; }
    #status { margin-top: 12px; }
    #log { margin-top: 12px; white-space: pre-wrap; background: #f6f6f6; padding: 12px; border-radius: 8px; min-height: 120px; }
    .small { font-size: 12px; opacity: 0.8; }
  </style>
</head>
<body>
  <h3>sca_run</h3>
  <div>
    <button id="runBtn">Start Qwen</button>
    <button id="micBtn" disabled>Mic ON</button>
    <button id="spkBtn" disabled>Speaker OFF</button>
  </div>
  <div id="status" class="small">Idle. Click 'Start Qwen' to initialize model.</div>
  <div id="log" class="small"></div>

<script>
(() => {
  const micBtn = document.getElementById('micBtn');
  const runBtn = document.getElementById('runBtn');
  const spkBtn = document.getElementById('spkBtn');
  const statusEl = document.getElementById('status');
  const logEl = document.getElementById('log');

  let audioCtx = null;
  let mediaStream = null;
  let sourceNode = null;
  let processorNode = null;
  let zeroGain = null;

  let ws = null;
  let sending = false;
  let micOn = false;
  let speakerOn = false;

  let playCtx = null;
  let pendingAudioMeta = null;

  // Server expects 16kHz mono PCM16LE by default (see config/default.toml)
  const TARGET_SR = 16000;

  function setStatus(s) { statusEl.textContent = s; }
  function log(s) {
    logEl.textContent = (logEl.textContent + s + "\n").slice(-4000);
    logEl.scrollTop = logEl.scrollHeight;
  }

  function downsampleBuffer(buffer, inSampleRate, outSampleRate) {
    if (outSampleRate === inSampleRate) return buffer;
    if (outSampleRate > inSampleRate) throw new Error('outSampleRate must be <= inSampleRate');

    const ratio = inSampleRate / outSampleRate;
    const newLen = Math.round(buffer.length / ratio);
    const result = new Float32Array(newLen);

    let offsetResult = 0;
    let offsetBuffer = 0;
    while (offsetResult < result.length) {
      const nextOffsetBuffer = Math.round((offsetResult + 1) * ratio);
      let accum = 0;
      let count = 0;
      for (let i = offsetBuffer; i < nextOffsetBuffer && i < buffer.length; i++) {
        accum += buffer[i];
        count++;
      }
      result[offsetResult] = count > 0 ? (accum / count) : 0;
      offsetResult++;
      offsetBuffer = nextOffsetBuffer;
    }
    return result;
  }

  function floatTo16BitPCM(floatBuf) {
    const out = new Int16Array(floatBuf.length);
    for (let i = 0; i < floatBuf.length; i++) {
      let s = Math.max(-1, Math.min(1, floatBuf[i]));
      out[i] = s < 0 ? s * 0x8000 : s * 0x7fff;
    }
    return out;
  }

  async function startMic() {
    if (micOn) return;

    mediaStream = await navigator.mediaDevices.getUserMedia({
      audio: {
        echoCancellation: true,
        noiseSuppression: true,
        autoGainControl: true,
        channelCount: 1,
      }
    });

    audioCtx = new (window.AudioContext || window.webkitAudioContext)();
    sourceNode = audioCtx.createMediaStreamSource(mediaStream);

    // ScriptProcessor is deprecated but still widely supported and minimal for a prototype.
    // bufferSize 4096 is a common tradeoff.
    processorNode = audioCtx.createScriptProcessor(4096, 1, 1);

    // Avoid echo/feedback by routing through a zero gain node.
    zeroGain = audioCtx.createGain();
    zeroGain.gain.value = 0;

    processorNode.onaudioprocess = (e) => {
      const input = e.inputBuffer.getChannelData(0);
      const inSR = audioCtx.sampleRate;
      const mono16k = downsampleBuffer(input, inSR, TARGET_SR);
      const pcm16 = floatTo16BitPCM(mono16k);

      if (sending && ws && ws.readyState === WebSocket.OPEN) {
        ws.send(pcm16.buffer);
      }
    };

    sourceNode.connect(processorNode);
    processorNode.connect(zeroGain);
    zeroGain.connect(audioCtx.destination);

    micOn = true;
    micBtn.textContent = 'Mic OFF';
    
    if (ws && ws.readyState === WebSocket.OPEN) {
        sending = true;
        setStatus(`Mic Streaming ON. Sending ${TARGET_SR}Hz mono PCM16.`);
    } else {
        setStatus(`Mic ON (Local). Waiting for Qwen connection...`);
    }
  }

  async function stopMic() {
    if (!micOn) return;

    try { processorNode && processorNode.disconnect(); } catch (e) {}
    try { sourceNode && sourceNode.disconnect(); } catch (e) {}
    try { zeroGain && zeroGain.disconnect(); } catch (e) {}

    if (mediaStream) {
      for (const t of mediaStream.getTracks()) t.stop();
    }

    if (audioCtx) {
      try { await audioCtx.close(); } catch (e) {}
    }

    audioCtx = null;
    mediaStream = null;
    sourceNode = null;
    processorNode = null;
    zeroGain = null;

    micOn = false;
    sending = false; // Stop sending
    micBtn.textContent = 'Mic ON';
    setStatus('Mic OFF. Streaming stopped.');
  }

  function wsUrl() {
    const proto = location.protocol === 'https:' ? 'wss' : 'ws';
    return `${proto}://${location.host}/ws/pcm16`;
  }

  function ensurePlayCtx() {
    if (!playCtx) {
      playCtx = new (window.AudioContext || window.webkitAudioContext)();
    }
    if (playCtx.state === 'suspended') {
      // Must be called from a user gesture (e.g. button click)
      playCtx.resume();
    }
  }

  function pcm16ToFloat32(pcm16) {
    const f = new Float32Array(pcm16.length);
    for (let i = 0; i < pcm16.length; i++) {
      f[i] = Math.max(-1, Math.min(1, pcm16[i] / 32768));
    }
    return f;
  }

  function playPcm16le(arrayBuffer, meta) {
    if (!speakerOn) return;
    if (!meta || meta.audio_format !== 'pcm16le') {
      log('Got binary audio but missing/unsupported meta.');
      return;
    }

    ensurePlayCtx();

    const pcm16 = new Int16Array(arrayBuffer);
    const floatBuf = pcm16ToFloat32(pcm16);

    const sr = meta.audio_sample_rate || 24000;
    const ch = meta.channels || 1;

    // For now we downmix on server, so expect mono.
    const audioBuffer = playCtx.createBuffer(1, floatBuf.length, sr);
    audioBuffer.getChannelData(0).set(floatBuf);

    const src = playCtx.createBufferSource();
    src.buffer = audioBuffer;
    src.connect(playCtx.destination);
    src.start(0);
  }

  function startQwen() {
    if (ws) return;

    ws = new WebSocket(wsUrl());
    ws.binaryType = 'arraybuffer';

    ws.onopen = () => {
      // connecting...
      sending = false; // Wait for server_ready
      runBtn.textContent = 'Stop Qwen';
      // Disable Run Button temporarily? No, allow stop.
      setStatus('Connected to Server. Initializing Qwen Model (Please wait)...');
      log('WS connected. Waiting for engine...');
    };

    ws.onmessage = (ev) => {
      // We can receive JSON (status/meta) or binary (PCM16LE).
      if (typeof ev.data === 'string') {
        try {
          const obj = JSON.parse(ev.data);
          if (obj && typeof obj === 'object') {
            // [Added] Check server_ready
            if (obj.type === "server_ready") {
                micBtn.disabled = false;
                spkBtn.disabled = false;
                setStatus('Model Initialized! Please click "Mic ON" to start speaking.');
                log(`âœ… ${obj.message}`);
                return;
            }

            if (obj.type === 'talker_audio') {
              pendingAudioMeta = obj;
              if (obj.text_log) log('talker: ' + obj.text_log);
              return;
            }

            if (obj.error) {
              log('ERROR: ' + obj.error);
            } else {
              let line = `chunk #${obj.chunks ?? '?'} processed`;
              if (obj.team_audio === false) line += ' | (team audio: not configured)';
              log(line);
            }
          } else {
            log(String(ev.data));
          }
        } catch {
          log(String(ev.data));
        }
        return;
      }

      // Binary -> play if speaker enabled
      if (ev.data instanceof ArrayBuffer) {
        playPcm16le(ev.data, pendingAudioMeta);
        pendingAudioMeta = null;
        return;
      }
    };

    ws.onclose = () => {
      sending = false;
      ws = null;
      runBtn.textContent = 'Start Qwen';
      micBtn.disabled = true;
      spkBtn.disabled = true;
      
      // Stop mic if it was running? Or keep it? Use case seems to require mic is slave to Qwen connection usually
      if (micOn) stopMic(); 
      
      setStatus('Disconnected.');
      log('WS closed.');
    };

    ws.onerror = () => {
      log('WS error.');
    };
  }

  function stopQwen() {
    if (!ws) return;
    try { ws.close(); } catch (e) {}
    // cleanup handles in onclose
  }

  micBtn.addEventListener('click', async () => {
    try {
      if (!micOn) {
          await startMic();
      } else {
          await stopMic();
      }
    } catch (e) {
      log('Mic error: ' + (e && e.message ? e.message : String(e)));
      setStatus('Mic error.');
    }
  });

  runBtn.addEventListener('click', () => {
    if (!ws) startQwen();
    else stopQwen();
  });

  spkBtn.addEventListener('click', () => {
    speakerOn = !speakerOn;
    if (speakerOn) {
      ensurePlayCtx();
      spkBtn.textContent = 'Speaker ON';
      log('Speaker ON.');
    } else {
      spkBtn.textContent = 'Speaker OFF';
      log('Speaker OFF.');
    }
  });
})();
</script>
</body>
</html>
