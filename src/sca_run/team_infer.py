from __future__ import annotations

"""Team inference integration point - Qwen3-Omni FullDuplex ê²°í•© ë²„ì „.

ì´ ëª¨ë“ˆì€ ë‹¤ìŒì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:
  1. feature_extractor.pyê°€ ìƒì„±í•œ Log-Mel Spectrogram [1, 128, T]ì„ ë°›ìŒ
  2. íŒ€ì›ì˜ Qwen3DuplexLogic(src/inference.py)ì— ì „ë‹¬
  3. Thinker(ì´í•´) â†’ Talker(ëŒ€ë‹µ) â†’ Code2Wav(ìŒì„± ìƒì„±) ì²˜ë¦¬
  4. ìƒì„±ëœ ìŒì„±ì„ TeamAudioReturnìœ¼ë¡œ ë°˜í™˜

ë°ì´í„° íë¦„:
  PCM16 ìŒì„± â†’ feature_extractor â†’ Log-Mel [1,128,T]
           â†’ team_infer.py (ì´ íŒŒì¼)
           â†’ Qwen3DuplexLogic
           â†’ ìŒì„± ìƒì„± [T]
           â†’ WebSocketìœ¼ë¡œ í´ë¼ì´ì–¸íŠ¸ì— ì „ë‹¬
"""

import os
import threading
import queue
from functools import lru_cache
from typing import Optional

import numpy as np
import torch

from utils.client_utils import log
from .config import AppConfig
from .io_types import AudioInput, TeamAudioReturn

# íŒ€ì›ì˜ ì½”ë“œ ì„í¬íŠ¸
try:
    # Qwen3OmniFullDuplexEngineì€ ìœ ì €ê°€ ì‘ì„±í•œ Engine í´ë˜ìŠ¤ (run_test.py ì°¸ì¡°)
    from inference import Qwen3OmniFullDuplexEngine, EngineConfig
    TEAM_CODE_AVAILABLE = True
except ImportError:
    TEAM_CODE_AVAILABLE = False
    log("warning", "[Warning] íŒ€ì›ì˜ inference.pyë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. src/inference.py ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")


def _env(key: str, default: str = "") -> str:
    v = os.getenv(key)
    return default if v is None else v


# ============================================================================
# ì „ì—­ ìƒíƒœ ê´€ë¦¬
# ============================================================================

_model_lock = threading.Lock()
_qwen_model = None
_qwen_tokenizer = None

def _load_qwen_model_and_tokenizer(cfg: AppConfig):
    """íŒ€ì›ì˜ íŒŒì¸íŠœë‹ëœ Qwen3-Omni ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    global _qwen_model, _qwen_tokenizer
    
    if _qwen_model is not None and _qwen_tokenizer is not None:
        return _qwen_model, _qwen_tokenizer
    
    log("info", "[Team Inference] ğŸ”„ Qwen3-Omni ëª¨ë¸ ë¡œë”© ì¤‘...")
    
    try:
        from transformers import AutoTokenizer, Qwen3OmniMoeForConditionalGeneration
        
        # Use Config object instead of raw env vars
        model_id = cfg.qwen.model_id
        device_map = cfg.qwen.device_map
        torch_dtype = cfg.qwen.torch_dtype
        attn_impl = cfg.qwen.attn_implementation

        log("info", f"[Team Inference] Model ID: {model_id}")
        log("info", f"[Team Inference] Device Map: {device_map}")
        if attn_impl:
            log("info", f"[Team Inference] Attention Implementation: {attn_impl}")
        
        # ëª¨ë¸ ë¡œë“œ (Qwen3OmniMoeForConditionalGeneration ì‚¬ìš©)
        _qwen_model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(
            model_id,
            device_map=device_map,
            torch_dtype=torch_dtype if torch_dtype != "auto" else None,
            trust_remote_code=True,
            attn_implementation=attn_impl,
        )
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        _qwen_tokenizer = AutoTokenizer.from_pretrained(
            model_id,
            trust_remote_code=True
        )

        log("info", "[Team Inference] âœ… ëª¨ë¸ & í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ!")
        return _qwen_model, _qwen_tokenizer
    
    except Exception as e:
        log("error", f"[Team Inference] âŒ ëª¨ë¸/í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise


# ============================================================================
# Session-based Inference (Engine Wrapper)
# ============================================================================

class TeamInferenceSession:
    """Per-connection session state using Qwen3OmniFullDuplexEngine."""

    def __init__(self, cfg: AppConfig):
        self.cfg = cfg
        self.model = None
        self.tokenizer = None
        self.processor = None
        self.engine = None
        self.started = False
    
    async def initialize(self):
        """Async initialization to prevent blocking the server event loop."""
        import asyncio
        loop = asyncio.get_running_loop()
        
        log("info", "[Team Inference] Starting async model loading...")

        # 1. Load Model & Tokenizer (Thread-safe, non-blocking for asyncio)
        def _load_mt():
            with _model_lock:
                return _load_qwen_model_and_tokenizer(self.cfg)
        
        self.model, self.tokenizer = await loop.run_in_executor(None, _load_mt)

        # 2. Load Processor
        def _load_proc():
            from transformers import Qwen3OmniMoeProcessor
            # Use local path if possible or download
            return Qwen3OmniMoeProcessor.from_pretrained(
                self.model.config._name_or_path, 
                trust_remote_code=True
            )
        
        if self.processor is None:
            self.processor = await loop.run_in_executor(None, _load_proc)
            log("info", "[Team Inference] âœ… Processor ë¡œë“œ ì™„ë£Œ!")

        # 3. Engine ì´ˆê¸°í™”
        if not TEAM_CODE_AVAILABLE:
            raise RuntimeError("src.inference not available")

        self.engine_config = EngineConfig(
            system_prompt_text=self.cfg.qwen.system_prompt
        )
        
        self.engine = Qwen3OmniFullDuplexEngine(
            model=self.model,
            tokenizer=self.tokenizer,
            config=self.engine_config
        )
        log("info", "[Team Inference] New engine session created.")

    async def start(self):
        """Start the inference engine."""
        if self.engine is None:
             await self.initialize()

        if not self.started:
            await self.engine.start()
            self.started = True
            log("info", "[Team Inference] Engine started.")

    async def stop(self):
        """Stop the inference engine."""
        if self.started:
            await self.engine.stop()
            self.started = False
            log("info", "[Team Inference] Engine stopped.")

    async def push_input(self, audio_in: AudioInput):
        """
        Push audio to the engine.
        
        [Strict Verification Updated]
        run_test.pyì™€ ë™ì¼í•˜ê²Œ Processorë¥¼ ì‚¬ìš©í•˜ì—¬ Feature Extraction ìˆ˜í–‰.
        audio_in.featuresëŠ” ì´ì œ Raw Float Tensor (1D) ë˜ëŠ” ê¸°ì¡´ Features (2D)ì¼ ìˆ˜ ìˆìŒ.
        """
        if not self.started:
            return

        # 1. ì…ë ¥ ë°ì´í„° í™•ì¸
        data = audio_in.features
        target_device = self.model.device
        target_dtype = self.model.dtype

        # 2. Raw Float Audioì¸ ê²½ìš° (Dimension Check)
        # [1, T] or [T] -> Raw Waveform
        # [1, 128, T] -> Pre-computed Mel (ê¸°ì¡´ ë°©ì‹)
        
        is_raw_audio = False
        if isinstance(data, torch.Tensor):
            if data.dim() <= 2 and data.shape[-2] != 128: 
                is_raw_audio = True
        elif isinstance(data, np.ndarray):
             if data.ndim <= 2 and data.shape[-2] != 128:
                is_raw_audio = True

        if is_raw_audio:
            # run_test.py ë¡œì§ ì ìš©
            # chunk = numpy array
            if isinstance(data, torch.Tensor):
                chunk = data.detach().cpu().numpy().squeeze()
            else:
                chunk = np.array(data).squeeze()
                
            # Padding Logic (Strictly following run_test.py)
            target_len = int(16000 * 0.64) # 10240 samples
            if len(chunk) < target_len:
                # pad right
                chunk = np.pad(chunk, (0, target_len - len(chunk)))
            
            # Feature Extraction via Processor
            features = self.processor.feature_extractor(
                [chunk], 
                return_tensors="pt", 
                sampling_rate=16000,
                padding=False,
            )
            input_features = features.input_features.to(target_device).to(target_dtype)
            
            # NaN Check
            if torch.isnan(input_features).any() or torch.isinf(input_features).any():
                input_features = torch.nan_to_num(input_features, nan=0.0, posinf=0.0, neginf=0.0)
                
            await self.engine.push_audio(input_features)
            
        else:
            # ê¸°ì¡´ Pre-computed Feature ê²½ë¡œ (Feature Extractor ì‚¬ìš© ì‹œ)
            # ë§Œì•½ server.pyê°€ ì—¬ì „íˆ qwen_clientì˜ log_mel_spectrogramì„ ì“´ë‹¤ë©´ ì´ë¦¬ë¡œ ì˜´.
            # í•˜ì§€ë§Œ run_test.pyì™€ ë§ì¶”ë ¤ë©´ Rawë¡œ ë³´ë‚´ëŠ”ê²Œ ë§ìŒ.
            if not isinstance(data, torch.Tensor):
                features = torch.from_numpy(data)
            else:
                features = data
            
            features = features.to(device=target_device, dtype=target_dtype)
            await self.engine.push_audio(features)

    async def get_output(self) -> Optional[TeamAudioReturn]:
        """
        Try to get audio output from the engine.
        Returns TeamAudioReturn or None.
        """
        if not self.started:
            return None

        # get_audio_output returns bytes (PCM16LE mono 24kHz presumably)
        out_bytes = await self.engine.get_audio_output()
        
        if out_bytes:
            # Convert bytes -> float32 [-1, 1]
            wav_int16 = np.frombuffer(out_bytes, dtype=np.int16)
            wav_float = wav_int16.astype(np.float32) / 32768.0
            
            # ì—”ì§„ ì¶œë ¥ ìƒ˜í”Œë ˆì´íŠ¸ëŠ” Qwen3 Omni ê¸°ë³¸ê°’ì¸ 24000Hzë¡œ ê°€ì •
            return TeamAudioReturn(
                wav=wav_float,
                sample_rate=24000,
                channels=1,
                text_log=None
            )
        return None

# ============================================================================
# Legacy / Single-shot Wrapper (Deprecated)
# ============================================================================



def infer_team_wav(cfg: AppConfig, audio_in: AudioInput) -> Optional[TeamAudioReturn]:

    """
    íŒ€ì›ì˜ Qwen3-Omni FullDuplex ëª¨ë¸ë¡œ ì¶”ë¡ í•©ë‹ˆë‹¤.
    
    ì…ë ¥:
        cfg: AppConfig (ì„¤ì •)
        audio_in: AudioInput (Log-Mel features [1, 128, T])
    
    ì¶œë ¥:
        TeamAudioReturn (wav float32, sample_rate=24000)
    """
    
    if not TEAM_CODE_AVAILABLE:
        print("[Team Inference] âš ï¸ íŒ€ì›ì˜ inference.pyë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    try:
        global _duplex_logic, _step_count
        
        # 1. Duplex Logic ì´ˆê¸°í™” (ì²˜ìŒ í•œ ë²ˆë§Œ)
        logic = _init_duplex_logic(cfg)
        
        # 2. ì…ë ¥ ë°ì´í„° ì¤€ë¹„
        features = audio_in.features
        
        # CPUì— ìˆìœ¼ë©´ ìœ ì§€, GPUì— ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ
        if isinstance(features, torch.Tensor):
            features = features.float()
        else:
            features = torch.from_numpy(features).float()
        
        print(f"[Team Inference] ì…ë ¥ Feature í˜•íƒœ: {features.shape}")
        
        # 3. Thinker ë‹¨ê³„: ì˜¤ë””ì˜¤ ì´í•´í•˜ê¸°
        print("[Team Inference] ğŸ§  Thinker ì²˜ë¦¬ ì¤‘...")
        
        # Feature Attention Mask ìƒì„±
        time_len = features.shape[2] if features.dim() == 3 else features.shape[1]
        feature_mask = torch.ones((1, time_len), dtype=torch.long)
        
        with torch.no_grad():
            # Thinker Step
            thinker_out = logic.thinker_step(
                input_ids=None,
                input_features=features,
                feature_attention_mask=feature_mask,
                past_key_values=None,
                step_idx=_step_count
            )
            
            _step_count += 1
            
            # ì²« í† í° ì˜ˆì¸¡
            next_token = thinker_out.logits[:, -1, :].argmax(dim=-1, keepdim=True)
            
            print(f"[Team Inference] Thinker ì˜ˆì¸¡ í† í°: {next_token.item()}")
            
            # 4. Talker ë‹¨ê³„: ë‹µë³€ ìƒì„±í•˜ê¸°
            print("[Team Inference] ğŸ‘„ Talker ì²˜ë¦¬ ì¤‘...")
            
            # Thinkerì˜ hidden stateë¥¼ ê°€ì ¸ì˜¤ê¸°
            thinker_hidden = thinker_out.hidden_states[-1]
            
            # Talker Step
            audio_codes, talker_kv = logic.talker_step(
                thinker_hidden=thinker_hidden,
                past_key_values=None,
                step_idx=_step_count,
                input_ids=None
            )
            
            _step_count += 1
            
            print(f"[Team Inference] ìƒì„±ëœ ì˜¤ë””ì˜¤ ì½”ë“œ í˜•íƒœ: {audio_codes.shape}")
            
            # 5. Code2Wav ë‹¨ê³„: ìŒì„± ìƒì„±í•˜ê¸°
            print("[Team Inference] ğŸµ Code2Wav ì²˜ë¦¬ ì¤‘...")
            
            wav_bytes = logic.decode_audio(audio_codes)
            
            # ë°”ì´íŠ¸ë¥¼ float32 ë°°ì—´ë¡œ ë³€í™˜
            wav_int16 = np.frombuffer(wav_bytes, dtype=np.int16)
            wav_float = wav_int16.astype(np.float32) / 32768.0
            wav_float = np.clip(wav_float, -1.0, 1.0)
            
            print(f"[Team Inference] âœ… ìƒì„±ëœ ìŒì„± ê¸¸ì´: {len(wav_float)} samples ({len(wav_float)/24000:.2f}ì´ˆ)")
            
            # 6. TeamAudioReturnìœ¼ë¡œ ë°˜í™˜
            return TeamAudioReturn(
                wav=wav_float,
                sample_rate=24000,  # Qwen3-Omniì˜ ê¸°ë³¸ ìƒ˜í”Œë ˆì´íŠ¸
                channels=1,
                text_log=None
            )
    
    except Exception as e:
        import traceback
        print(f"[Team Inference] âŒ ì¶”ë¡  ì‹¤íŒ¨: {e}")
        traceback.print_exc()
        return None


def reset_conversation():
    """ëŒ€í™” ìƒíƒœë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤ (ìƒˆë¡œìš´ ëŒ€í™” ì‹œì‘)."""
    global _duplex_logic, _step_count
    
    with _model_lock:
        _duplex_logic = None
        _step_count = 0
        print("[Team Inference] ğŸ”„ ëŒ€í™” ìƒíƒœ ì´ˆê¸°í™”ë¨")
