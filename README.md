# sca_run

ssh -L 8080:localhost:8080 root@38.128.232.57 -p 19542 -i ~/.ssh/id_ed25519

Minimal scaffold for **streaming audio -> features -> Qwen3-Omni (Transformers) -> text**.

Team request addressed:
- remove hardcoded `12.5Hz / 4` audio splitting
- make audio chunking configurable (TOML + env overrides)

Architecture note:
- The server can precompute audio features (mel-style `input_features`) and pass
  them into the inference step via `AudioInput`. This makes it easy to plug in a
  separate "thinker/talker" module that expects features instead of raw audio.

## 1) Config

Edit: `config/default.toml`

Key knobs:
- `audio.frame_hz` (default `12.5`)  
  - 12.5Hz => 80ms per frame
- `audio.frames_per_chunk` (default `4`)  
  - 4 frames => 320ms per request

You can override without touching code:
- `SCA_FRAME_HZ`
- `SCA_FRAMES_PER_CHUNK`

Qwen (Transformers) settings:
- `qwen.model_id` / `SCA_QWEN_MODEL_ID`
- `qwen.device_map` / `SCA_QWEN_DEVICE_MAP`
- `qwen.torch_dtype` / `SCA_QWEN_TORCH_DTYPE`
- `qwen.attn_implementation` / `SCA_QWEN_ATTN_IMPL`
- `qwen.max_new_tokens` / `SCA_QWEN_MAX_NEW_TOKENS`

> Note: /infer_wav uses Python's built-in WAV decoder, so it supports *uncompressed* 16-bit PCM WAV. For other formats, decode on the client side first.

## 2) Run

```bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt

# example: local HF model id
export SCA_QWEN_MODEL_ID="Qwen/Qwen3-Omni-30B-A3B-Instruct"
export SCA_QWEN_DEVICE_MAP="auto"      # or "cuda:0"
export SCA_QWEN_TORCH_DTYPE="auto"     # or "float16"

python -m sca_run.server --config config/default.toml --host 0.0.0.0 --port 8000
import torch
from dataclasses import dataclass
from typing import Optional

```bash
@dataclass
class AudioInput:
 
    # 전처리된 Mel-Spectrogram Feature [1, 128, T]
    features: torch.Tensor 
    
    # (선택) 디버깅용 타임스탬프 (Lag 측정용)
    timestamp: float = 0.0

@dataclass
class ThoughtPacket:
 
    # Talker의 입력이 될 Hidden States [1, Seq, Dim]
    hidden_states: torch.Tensor
    
    # 예: "음...", "반갑", "습니다"
    text_token_str: Optional[str] = None

@dataclass
class AudioOutput:
  
    # 스피커로 재생할 PCM Audio Bytes (Int16)
    audio_bytes: bytes
    
    # (선택) 이 오디오가 어떤 텍스트에서 나왔는지 (자막용)
    text_log: Optional[str] = None
```

## 3) Endpoints

### Health
- `GET /health` => "ok"

### One-shot WAV inference
- `POST /infer_wav?prompt=...` with multipart form field `file` (wav)

### Streaming PCM16 WebSocket
- `WS /ws/pcm16`

Protocol:
1) (optional) first message: **text JSON** for session overrides
   - `{"prompt": "..."}`
   - `{"frames_per_chunk": 6}`
   - `{"frame_hz": 12.5}`
2) then send **binary** messages: little-endian **mono PCM16** bytes
3) server converts PCM -> features (AudioInput) -> inference, and replies **JSON** per chunk:
   - `{ "text": "...", "chunk_ms": 320.0, "frames_per_chunk": 4, "frame_hz": 12.5 }`
