# scripts/run_test.py
import os
import sys
import time
import argparse
import torch
import numpy as np
import librosa
import soundfile as sf
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

# íŒ¨í‚¤ì§€ ê²½ë¡œ (sca_core)
from src.inference import Qwen3OmniFullDuplexEngine, EngineConfig
from transformers import Qwen3OmniMoeForConditionalGeneration, Qwen3OmniMoeProcessor

def load_audio_file(file_path, target_sr=24000):
    """ì˜¤ë””ì˜¤ íŒŒì¼ì„ ë¡œë“œí•˜ê³  ë¦¬ìƒ˜í”Œë§í•¨"""
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"Audio file not found: {file_path}")
        
    print(f"ğŸ“‚ Loading audio file: {file_path}")
    # librosaëŠ” float32 [-1, 1]ë¡œ ë¡œë“œí•¨
    audio, sr = librosa.load(file_path, sr=target_sr, mono=True)
    return audio, sr

def main():
    parser = argparse.ArgumentParser(description="Test Full-Duplex Engine with Audio File")
    parser.add_argument("--model-path", type=str, default="Qwen/Qwen3-Omni-30B-A3B-Instruct", help="Path to model")
    parser.add_argument("--input-file", type=str, required=True, help="Input audio file (e.g. 3min_noisy.wav)")
    parser.add_argument("--output-file", type=str, default="output_response.wav", help="Output audio file")
    parser.add_argument("--device", type=str, default="cuda", help="Device to run")
    args = parser.parse_args()

    # 1. ëª¨ë¸ ë¡œë“œ
    print(f"ğŸ”¥ Loading Model from {args.model_path}...")
    
    # A40 2ì¥ì´ë©´ "auto", 1ì¥ì´ë©´ "cuda:0" ë“± ìƒí™©ì— ë§ê²Œ ì„¤ì •
    device_map = "auto"
    
    model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(
        args.model_path,
        device_map=device_map, 
        dtype='auto',          
        attn_implementation='flash_attention_2', 
        trust_remote_code=True
    )
    
    # 3. í”„ë¡œì„¸ì„œ ë¡œë“œ
    processor = Qwen3OmniMoeProcessor.from_pretrained(args.model_path, trust_remote_code=True)
    
    # 2. ì—”ì§„ ì´ˆê¸°í™” (processor.tokenizer ì „ë‹¬)
    config = EngineConfig(audio_input_tokens=4, text_output_tokens=2, audio_output_tokens=4)
    engine = Qwen3OmniFullDuplexEngine(model, processor.tokenizer, config)
    
    # 3. ì˜¤ë””ì˜¤ ì¤€ë¹„ (Chunking)
    full_audio, sr = load_audio_file(args.input_file, target_sr=16000)
    INPUT_SR = 16000
    # 4í† í° ë¶„ëŸ‰ì˜ ì˜¤ë””ì˜¤ ê¸¸ì´ ê³„ì‚° (0.32ì´ˆ)
    # 24000 * 0.32 = 7680 samples
    chunk_size = int(sr * 0.32) 
    
    chunks = [full_audio[i:i + chunk_size] for i in range(0, len(full_audio), chunk_size)]
    print(f"ğŸ“¦ Audio split into {len(chunks)} chunks (Chunk size: {chunk_size} samples)")

    # 4. í…ŒìŠ¤íŠ¸ ì‹œì‘ (ì“°ë ˆë“œ ê°€ë™)
    engine.start()
    
    collected_output_audio = []
    start_time = time.time()
    
    try:
        for i, chunk in enumerate(chunks):
            # ë§ˆì§€ë§‰ ì§œíˆ¬ë¦¬ íŒ¨ë”© (0ìœ¼ë¡œ ì±„ì›€)
            if len(chunk) < chunk_size:
                chunk = np.pad(chunk, (0, chunk_size - len(chunk)))
            
            with torch.no_grad():
                # â˜… [í•µì‹¬ ìˆ˜ì • 1] Feature Extractor í˜¸ì¶œ ì‹œ padding=False ì„¤ì •
                # WhisperëŠ” ê¸°ë³¸ì ìœ¼ë¡œ 30ì´ˆ(3000í”„ë ˆì„)ë¡œ íŒ¨ë”©í•˜ëŠ”ë°, 
                # ì§§ì€ ìŠ¤íŠ¸ë¦¬ë° ì²­í¬ë¥¼ ë„£ì„ ë•ŒëŠ” íŒ¨ë”©ì„ ë„ê±°ë‚˜ ê¸¸ì´ë¥¼ ë§ì¶°ì•¼ í•¨.
                # ì—¬ê¸°ì„œëŠ” Qwen3-Omniê°€ ì§§ì€ ì…ë ¥ì„ í—ˆìš©í•œë‹¤ê³  ê°€ì •í•˜ê³  íŒ¨ë”© ì—†ì´ ë³€í™˜.
                
                processed_inputs = processor.feature_extractor(
                    [chunk], 
                    return_tensors="pt", 
                    sampling_rate=INPUT_SR,
                    padding="longest",  # ë°°ì¹˜ 1ê°œë¼ ì˜ë¯¸ ì—†ì§€ë§Œ ëª…ì‹œ
                    do_normalize=True
                )
                
                # â˜… [í•µì‹¬ ìˆ˜ì • 2] ì…ë ¥ í…ì„œì˜ Transpose ë¬¸ì œ í•´ê²°
                # ì—ëŸ¬ ë©”ì‹œì§€ì˜ 3000ì€ Whisperì˜ ê³ ì • ê¸¸ì´(30ì´ˆ) íŒ¨ë”© ê²°ê³¼ì¼ ìˆ˜ ìˆìŒ.
                # ì—¬ê¸°ì„œëŠ” ëª…ì‹œì ìœ¼ë¡œ ìŠ¬ë¼ì´ì‹±í•˜ì—¬ ì‹¤ì œ ê¸¸ì´ë§Œí¼ë§Œ ë„£ê±°ë‚˜,
                # ëª¨ë¸ì´ ë‚´ë¶€ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ë„ë¡ ë†”ë‘¬ì•¼ í•¨.
                
                # í•˜ì§€ë§Œ, Qwen3-OmniëŠ” ìŠ¤íŠ¸ë¦¬ë° ì‹œ ê³ ì • ê¸¸ì´ ì…ë ¥ì„ ê°€ì •í•  ìˆ˜ ìˆìŒ.
                # ë§Œì•½ ìœ„ ì—ëŸ¬ê°€ "3000"ì´ ì…ë ¥ í¬ê¸°ë¼ê³  í–ˆë‹¤ë©´, 
                # feature_extractorê°€ ì´ë¯¸ 3000ìœ¼ë¡œ íŒ¨ë”©í•´ì„œ ì¤¬ë‹¤ëŠ” ëœ»ì„.
                
                input_features = processed_inputs.input_features.to(args.device).to(model.dtype)
                # [Batch, Mel, Time] -> [1, 128, 3000] (Whisper ê¸°ë³¸ ë™ì‘)

                # feature_lens: ì‹¤ì œ ìœ íš¨í•œ ê¸¸ì´ ê³„ì‚° (ì „ì²´ í”„ë ˆì„ ìˆ˜ ì•„ë‹˜)
                # 0.32ì´ˆ ì˜¤ë””ì˜¤ëŠ” ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ìƒ ì•½ 32í”„ë ˆì„ ì •ë„ ë¨.
                # í•˜ì§€ë§Œ ëª¨ë¸ì´ 3000ì„ ë°›ì•˜ë‹¤ë©´ feature_lensë„ 3000ìœ¼ë¡œ ë§ì¶°ì¤˜ì•¼ í•  ìˆ˜ ìˆìŒ.
                # ì—¬ê¸°ì„œëŠ” processorê°€ ê³„ì‚°í•´ì¤€ attention_maskë¥¼ ë¯¿ìŒ.
                
                if hasattr(processed_inputs, "attention_mask") and processed_inputs.attention_mask is not None:
                    # attention_maskê°€ ìˆë‹¤ë©´ ìœ íš¨ ê¸¸ì´ í•©ê³„ ì‚¬ìš©
                    feature_lens = processed_inputs.attention_mask.sum(1).to(args.device)
                else:
                    # ì—†ë‹¤ë©´, ìš°ë¦¬ê°€ ë„£ì€ ì˜¤ë””ì˜¤ ê¸¸ì´ì— ë¹„ë¡€í•´ì„œ ê³„ì‚° (Whisper: 16000Hz -> 100Hz frame rate)
                    # 5120 ìƒ˜í”Œ / 160 = 32 í”„ë ˆì„
                    # í•˜ì§€ë§Œ ëª¨ë¸ì´ 3000ìœ¼ë¡œ íŒ¨ë”©ëœ ê±¸ ë°›ìœ¼ë©´ ì—ëŸ¬ê°€ ë‚  ìˆ˜ ìˆìŒ.
                    # ì•ˆì „í•˜ê²Œ ì…ë ¥ í…ì„œì˜ ì‹¤ì œ ë§ˆì§€ë§‰ ì°¨ì› í¬ê¸°ë¥¼ ì‚¬ìš©.
                    feature_lens = torch.tensor([input_features.shape[2]], device=args.device)

                # 2. Audio Tower í˜¸ì¶œ
                audio_outputs = model.thinker.audio_tower(
                    input_features, 
                    feature_lens=feature_lens
                )
                
                audio_embeds = audio_outputs.last_hidden_state
            
            engine.push_audio(audio_embeds)
            
            # ìˆ˜ì‹  ë£¨í”„
            while True:
                out_bytes = engine.get_audio_output()
                if out_bytes is None: break
                
                out_np = np.frombuffer(out_bytes, dtype=np.int16).astype(np.float32) / 32767.0
                collected_output_audio.append(out_np)
                print(f"ğŸ”Š Received output chunk ({len(out_np)} samples) at step {i}")

        print("â³ Waiting for remaining outputs...")
        time.sleep(3.0)
        
        # ë‚¨ì€ê±° ì‹¹ ê¸ì–´ëª¨ìœ¼ê¸°
        while True:
            out_bytes = engine.get_audio_output()
            if out_bytes is None: break
            out_np = np.frombuffer(out_bytes, dtype=np.int16).astype(np.float32) / 32767.0
            collected_output_audio.append(out_np)

    except KeyboardInterrupt:
        print("ğŸ›‘ Test interrupted")
    except Exception as e:
        print(f"âŒ Error occurred: {e}")
        import traceback
        traceback.print_exc()
    finally:
        engine.stop()
    
    # 5. ê²°ê³¼ ì €ì¥
    if collected_output_audio:
        final_audio = np.concatenate(collected_output_audio)
        print(f"ğŸ’¾ Saving {len(final_audio)} samples ({len(final_audio)/24000:.1f}s) to {args.output_file}")
        sf.write(args.output_file, final_audio, 24000)
    else:
        print("âš ï¸ No audio generated! (Check inputs or silence logic)")

    print(f"âœ… Test Finished. Total time: {time.time() - start_time:.2f}s")

if __name__ == "__main__":
    main()