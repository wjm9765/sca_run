# Qwen3-Omni FullDuplex í†µí•© ê°€ì´ë“œ

## ğŸ“‹ ë³€ê²½ ì‚¬í•­ ìš”ì•½

### 1. `sca_run/team_infer.py` ì „ì²´ ìˆ˜ì •
**ê¸°ì¡´:** ë¹„ì–´ìˆë˜ í•¨ìˆ˜
**ë³€ê²½:** íŒ€ì›ì˜ Qwen3DuplexLogicê³¼ í†µí•©

**í•µì‹¬ ê¸°ëŠ¥:**
- Qwen3-Omni ëª¨ë¸ ë¡œë“œ
- Thinker(ì˜¤ë””ì˜¤ ì´í•´) ì‹¤í–‰
- Talker(ë‹µë³€ ìƒì„±) ì‹¤í–‰  
- Code2Wav(ìŒì„± ìƒì„±) ì‹¤í–‰
- Log-Mel Spectrogram â†’ ìŒì„± ë³€í™˜

### 2. ì „ì—­ ìƒíƒœ ê´€ë¦¬ ì¶”ê°€
```python
_model_lock = threading.Lock()          # ë©€í‹°ìŠ¤ë ˆë“œ ì•ˆì „ì„±
_qwen_model = None                      # ë¡œë“œëœ ëª¨ë¸
_duplex_logic = None                    # Qwen3DuplexLogic ì¸ìŠ¤í„´ìŠ¤
_step_count = 0                         # ì¶”ë¡  ë‹¨ê³„ ì¹´ìš´í„°
```

---

## ğŸ”§ ì„¤ì • ë°©ë²•

### ë°©ë²• 1: í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (ê¶Œì¥)

**PowerShellì—ì„œ:**
```powershell
# íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ê²½ë¡œ
$env:SCA_QWEN_MODEL_ID = "path/to/your/finetuned/model"

# ë˜ëŠ” HuggingFace ëª¨ë¸
$env:SCA_QWEN_MODEL_ID = "Qwen/Qwen3-Omni-30B-A3B-Instruct"

# GPU ì„¤ì •
$env:SCA_QWEN_DEVICE_MAP = "cuda:0"

# ë°ì´í„° íƒ€ì… (ë©”ëª¨ë¦¬ ì ˆì•½)
$env:SCA_QWEN_TORCH_DTYPE = "float16"

# ì„œë²„ ì‹¤í–‰
python -m sca_run.server --config config/default.toml
```

**Linux/Macì—ì„œ:**
```bash
export SCA_QWEN_MODEL_ID="path/to/your/finetuned/model"
export SCA_QWEN_DEVICE_MAP="auto"
export SCA_QWEN_TORCH_DTYPE="float16"

python -m sca_run.server --config config/default.toml
```

### ë°©ë²• 2: `config/default.toml` ìˆ˜ì •

```toml
[qwen]
backend = "team"
model_id = "path/to/your/finetuned/model"
device_map = "cuda:0"
torch_dtype = "float16"
```

---

## ğŸ“Š ë°ì´í„° íë¦„

```
ì›¹ UI (index.html)
   â†“ PCM16 ìŒì„± (16kHz)
   
server.py (/ws/pcm16)
   â†“ ì²­í¬ëœ PCM16
   
audio_chunker.py
   â†“ ê³ ì • í¬ê¸° PCM16 (320ms)
   
feature_extractor.py
   â†“ Log-Mel Spectrogram [1, 128, T]
   
qwen_client.py
   â†“ AudioInput(features)
   
team_infer.py (âœ¨ ìƒˆë¡œ êµ¬í˜„)
   â”œâ”€ _load_qwen_model()
   â”‚  â””â”€ íŒ€ì›ì˜ íŒŒì¸íŠœë‹ ëª¨ë¸ ë¡œë“œ
   â”‚
   â”œâ”€ _init_duplex_logic()
   â”‚  â””â”€ Qwen3DuplexLogic ì´ˆê¸°í™”
   â”‚
   â””â”€ infer_team_wav()
      â”œâ”€ thinker_step() â†’ ì˜¤ë””ì˜¤ ì´í•´ [1, 128, T] â†’ í…ìŠ¤íŠ¸ í† í°
      â”œâ”€ talker_step()  â†’ í…ìŠ¤íŠ¸ â†’ ì˜¤ë””ì˜¤ ì½”ë“œ
      â”œâ”€ code2wav()     â†’ ì˜¤ë””ì˜¤ ì½”ë“œ â†’ ìŒì„± [T]
      â””â”€ float32ë¡œ ë³€í™˜
      
qwen_client.py
   â†“ TeamAudioReturn(wav float32, sr=24000)
   
server.py
   â†“ PCM16LEë¡œ ë³€í™˜
   
index.html (âœ¨ ìŠ¤í”¼ì»¤)
   â””â”€ ì‹¤ì‹œê°„ ì¬ìƒ!
```

---

## ğŸš€ ì‹¤í–‰ ë°©ë²•

### 1ë‹¨ê³„: í†µí•© í™•ì¸
```bash
python setup_integration.py
```

ì´ ìŠ¤í¬ë¦½íŠ¸ê°€ í™•ì¸í•˜ëŠ” ê²ƒ:
- âœ… íŒ€ì›ì˜ ì½”ë“œ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€
- âœ… í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì—¬ë¶€
- âœ… í•„ìš”í•œ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

### 2ë‹¨ê³„: ëª¨ë¸ ë¡œë“œ ë° ì„œë²„ ì‹œì‘
```bash
python -m sca_run.server --config config/default.toml --host 0.0.0.0 --port 8000
```

### 3ë‹¨ê³„: ì›¹ ë¸Œë¼ìš°ì €ì—ì„œ ì ‘ì†
```
http://localhost:8000
```

### 4ë‹¨ê³„: ë§ˆì´í¬ë¡œ ë§í•˜ë©´ AIê°€ ë‹µë³€!

---

## ğŸ“Š í•¨ìˆ˜ êµ¬ì¡°

### `team_infer.py` ë‚´ë¶€ í•¨ìˆ˜ë“¤

#### 1. `_load_qwen_model(cfg: AppConfig) -> torch.nn.Module`
**ì—­í• :** Qwen3-Omni ëª¨ë¸ ë¡œë“œ
**ì…ë ¥:** AppConfig
**ì¶œë ¥:** ë¡œë“œëœ ëª¨ë¸
**ì²˜ë¦¬:**
```python
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map=device_map,
    torch_dtype=torch_dtype,
    trust_remote_code=True,
)
```

#### 2. `_init_duplex_logic(cfg: AppConfig) -> Qwen3DuplexLogic`
**ì—­í• :** Qwen3DuplexLogic ì´ˆê¸°í™” (í•œ ë²ˆë§Œ)
**ì…ë ¥:** AppConfig
**ì¶œë ¥:** íŒ€ì›ì˜ Qwen3DuplexLogic ì¸ìŠ¤í„´ìŠ¤
**ì²˜ë¦¬:**
```python
model = _load_qwen_model(cfg)  # ëª¨ë¸ ë¡œë“œ
logic = Qwen3DuplexLogic(model)  # íŒ€ì›ì˜ í´ë˜ìŠ¤ ì‚¬ìš©
```

#### 3. `infer_team_wav(cfg: AppConfig, audio_in: AudioInput) -> Optional[TeamAudioReturn]`
**ì—­í• :** ì‹¤ì œ ì¶”ë¡  ìˆ˜í–‰ (ë©”ì¸ í•¨ìˆ˜)
**ì…ë ¥:** 
- `cfg`: ì„¤ì •
- `audio_in`: AudioInput(features=[1,128,T])

**ì¶œë ¥:** TeamAudioReturn(wav float32, sr=24000)

**ì²˜ë¦¬ ìˆœì„œ:**
```
1. Duplex Logic ì´ˆê¸°í™”
2. Feature ì¤€ë¹„ (CPU/GPU ì²´í¬)
3. Thinker ì‹¤í–‰: ì˜¤ë””ì˜¤ ì´í•´
4. Talker ì‹¤í–‰: ë‹µë³€ ìƒì„±
5. Code2Wav ì‹¤í–‰: ìŒì„± ìƒì„±
6. ë°˜í™˜
```

#### 4. `reset_conversation()`
**ì—­í• :** ëŒ€í™” ìƒíƒœ ì´ˆê¸°í™”
**ì‚¬ìš©:** ìƒˆë¡œìš´ ëŒ€í™” ì‹œì‘ ì‹œ

---

## âš ï¸ ì£¼ì˜ì‚¬í•­

### 1. ì²« ë²ˆì§¸ í˜¸ì¶œì´ ëŠë¦° ì´ìœ 
```
ì²« ë²ˆì§¸ í˜¸ì¶œ:
  ëª¨ë¸ ë¡œë“œ (5~30ì´ˆ) â†’ Duplex Logic ì´ˆê¸°í™” â†’ ì¶”ë¡ 
  
ì´í›„ í˜¸ì¶œ:
  ì¶”ë¡ ë§Œ (1~2ì´ˆ)
```
âœ… ì´ê²ƒì€ ì •ìƒì…ë‹ˆë‹¤. ëŒ€í™”ë¥¼ ì‹œì‘í•˜ë©´ ì ì  ë¹¨ë¼ì§‘ë‹ˆë‹¤.

### 2. GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ
```powershell
# ë°ì´í„° íƒ€ì…ì„ float16ìœ¼ë¡œ ë³€ê²½
$env:SCA_QWEN_TORCH_DTYPE = "float16"

# ë˜ëŠ” 8-bit ì–‘ìí™”
$env:SCA_QWEN_LOAD_IN_8BIT = "true"
```

### 3. ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠëŠ” ê²½ìš°
```bash
# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ í…ŒìŠ¤íŠ¸
python -c "from transformers import AutoModelForCausalLM; AutoModelForCausalLM.from_pretrained('Qwen/Qwen3-Omni-30B-A3B-Instruct', trust_remote_code=True)"
```

---

## ğŸ” ë””ë²„ê¹…

### ë¡œê·¸ í™•ì¸
```bash
# ì „ì²´ ë¡œê·¸ í™•ì¸
python -m sca_run.server 2>&1 | Tee-Object -FilePath debug.log

# ë˜ëŠ” linux
python -m sca_run.server 2>&1 | tee debug.log
```

### Team Inference ë¡œê·¸ ë©”ì‹œì§€
```
[Team Inference] ğŸ”„ Qwen3-Omni ëª¨ë¸ ë¡œë”© ì¤‘...
[Team Inference] Model ID: ...
[Team Inference] âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!
[Team Inference] âœ… Qwen3DuplexLogic ì´ˆê¸°í™” ì™„ë£Œ!
[Team Inference] ì…ë ¥ Feature í˜•íƒœ: torch.Size([1, 128, T])
[Team Inference] ğŸ§  Thinker ì²˜ë¦¬ ì¤‘...
[Team Inference] ğŸ‘„ Talker ì²˜ë¦¬ ì¤‘...
[Team Inference] ğŸµ Code2Wav ì²˜ë¦¬ ì¤‘...
[Team Inference] âœ… ìƒì„±ëœ ìŒì„± ê¸¸ì´: ... samples (...ì´ˆ)
```

---

## ğŸ“ˆ ì„±ëŠ¥ ìµœì í™”

### 1. ë°°ì¹˜ ì²˜ë¦¬
í˜„ì¬ ì½”ë“œëŠ” í•œ ë²ˆì— 1ê°œì˜ ì²­í¬ë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
```python
# ì—¬ëŸ¬ ì²­í¬ë¥¼ ëª¨ì•„ì„œ ì²˜ë¦¬í•˜ë ¤ë©´ (ë¯¸êµ¬í˜„)
features_batch = torch.cat([f1, f2, f3], dim=0)
```

### 2. ìŠ¤íŠ¸ë¦¬ë° ìµœì í™”
íŒ€ì›ì˜ `src/interface.py`ì˜ `thinker_output_queue`ë¥¼ í™œìš©:
```python
# Thinker ê²°ê³¼ë¥¼ Queueì— ë„£ê¸°
state.thinker_output_queue.put(thinker_out.hidden_states[-1])

# Talkerê°€ Queueì—ì„œ êº¼ë‚´ê¸°
while not state.thinker_output_queue.empty():
    hidden = state.thinker_output_queue.popleft()
    # ... Talker ì²˜ë¦¬
```

---

## ğŸ“ ì¶”ê°€ ì„¤ì •

### `config/default.toml` ì „ì²´ ì„¤ì •
```toml
[audio]
sample_rate = 16000
frame_hz = 12.5
frames_per_chunk = 4
channels = 1
sample_width_bytes = 2

[qwen]
backend = "team"
model_id = "your/finetuned/model"
device_map = "cuda:0"
torch_dtype = "float16"
```

---

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„

### 1ë‹¨ê³„: í†µí•© í™•ì¸
```bash
python setup_integration.py
```

### 2ë‹¨ê³„: ëª¨ë¸ ê²½ë¡œ ì„¤ì •
```bash
$env:SCA_QWEN_MODEL_ID = "your_model_path"
```

### 3ë‹¨ê³„: ì„œë²„ ì‹œì‘
```bash
python -m sca_run.server --config config/default.toml
```

### 4ë‹¨ê³„: í…ŒìŠ¤íŠ¸
ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:8000 ì ‘ì†

---

## ğŸ’¡ ë¬¸ì œ í•´ê²°

### "íŒ€ì›ì˜ inference.pyë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
```
í•´ê²°: src/inference.pyê°€ sca_runê³¼ ê°™ì€ í´ë” êµ¬ì¡°ì— ìˆëŠ”ì§€ í™•ì¸
í˜„ì¬: sca_run/
      src/
          inference.py  âœ…
          interface.py  âœ…
```

### "CUDA out of memory"
```
í•´ê²°: float16 ì‚¬ìš©
$env:SCA_QWEN_TORCH_DTYPE = "float16"
```

### "ëª¨ë¸ì´ ë„ˆë¬´ ëŠë¦¼"
```
í•´ê²°: ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš©
$env:SCA_QWEN_MODEL_ID = "Qwen/Qwen3-Omni-10B-Instruct"
```

---

## ğŸ“ ì§€ì›

ë¬¸ì œê°€ ë°œìƒí•˜ë©´:
1. `setup_integration.py` ì‹¤í–‰ìœ¼ë¡œ ìƒíƒœ í™•ì¸
2. ë””ë²„ê·¸ ë¡œê·¸ í™•ì¸
3. í™˜ê²½ ë³€ìˆ˜ ì¬í™•ì¸

